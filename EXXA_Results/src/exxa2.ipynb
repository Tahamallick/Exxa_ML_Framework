{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJPMT44S0oSA",
        "outputId": "e5cdae07-4a25-4b59-9779-f376a21f538c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0Z7oWPB2WJa",
        "outputId": "c47dd8f9-2a29-471f-b803-76a5c1cfe7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from astropy.io import fits\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pytorch_msssim import ssim, ms_ssim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "T7o4DFI7MkFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = '/content/drive/MyDrive/EXXA_Results'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'training_progress'), exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'checkpoints'), exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'metrics'), exist_ok=True)"
      ],
      "metadata": {
        "id": "d3gx_rbIZt_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "3BOrrTn7Mold"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQa_F7ykMtel",
        "outputId": "0bbebe1a-58e5-4249-e953-9989e51a6816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "FqGfug6uMzwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiskDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.fits')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
        "        with fits.open(file_path) as hdul:\n",
        "            image = hdul[0].data[0]\n",
        "\n",
        "        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
        "\n",
        "        if len(image.shape) > 2:\n",
        "            image = image.squeeze()\n",
        "\n",
        "        image = image[np.newaxis, :, :]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return torch.FloatTensor(image)\n"
      ],
      "metadata": {
        "id": "d2hw4IMfMwNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised Clustering Model"
      ],
      "metadata": {
        "id": "gBBO3TJ2M_ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiskClustering:\n",
        "    def __init__(self, n_clusters=5):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def preprocess_images(self, images):\n",
        "        # Flatten images and normalize\n",
        "        flattened = images.reshape(images.shape[0], -1)\n",
        "        return self.scaler.fit_transform(flattened)\n",
        "\n",
        "    def fit(self, images):\n",
        "        processed = self.preprocess_images(images)\n",
        "        self.kmeans.fit(processed)\n",
        "        return self\n",
        "\n",
        "    def predict(self, images):\n",
        "        processed = self.preprocess_images(images)\n",
        "        return self.kmeans.predict(processed)\n",
        "\n",
        "    def visualize_clusters(self, images, labels):\n",
        "        fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i in range(self.n_clusters):\n",
        "            cluster_images = images[labels == i]\n",
        "            if len(cluster_images) > 0:\n",
        "                axes[i].imshow(cluster_images[0], cmap='viridis')\n",
        "                axes[i].set_title(f'Cluster {i}')\n",
        "                axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "apAxgC2FM76V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoencoder Model"
      ],
      "metadata": {
        "id": "_tKdqTOqNGi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiskAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DiskAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 75 * 75, latent_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128 * 75 * 75),\n",
        "            nn.Unflatten(1, (128, 75, 75)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        return self.decode(z)"
      ],
      "metadata": {
        "id": "rd-SezcqNDtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transit Curve Classifier"
      ],
      "metadata": {
        "id": "PjTQ9TOjNQih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransitClassifier(nn.Module):\n",
        "    def __init__(self, input_size=100):\n",
        "        super(TransitClassifier, self).__init__()\n",
        "\n",
        "        # Input normalization\n",
        "        self.input_norm = nn.BatchNorm1d(input_size)\n",
        "\n",
        "        # Simplified and effective feature extraction\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Simple attention\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_norm(x)\n",
        "        features = self.features(x)\n",
        "        attention = self.attention(features)\n",
        "        weighted_features = features * attention\n",
        "        return self.classifier(weighted_features)\n"
      ],
      "metadata": {
        "id": "VO72j3USNNOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef generate_transit_curves(n_samples=10000, n_points=100):\n",
        "    curves = np.zeros((n_samples, n_points))\n",
        "    labels = np.zeros(n_samples)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        t = np.linspace(0, 1, n_points)\n",
        "        if np.random.random() > 0.5:  # Transit present\n",
        "            # Clearer transit parameters\n",
        "            depth = np.random.uniform(0.03, 0.15)  # Deeper transits\n",
        "            duration = np.random.uniform(0.1, 0.2)  # More consistent duration\n",
        "            center = np.random.uniform(0.3, 0.7)  # More centered transits\n",
        "\n",
        "            # Create transit\n",
        "            transit = np.ones_like(t)\n",
        "            mask = np.abs(t - center) < duration/2\n",
        "            transit[mask] = 1 - depth\n",
        "\n",
        "            # Reduced noise\n",
        "            noise = np.random.normal(0, 0.001, len(t))  # Less noise\n",
        "            curve = transit + noise\n",
        "\n",
        "            curves[i] = curve\n",
        "            labels[i] = 1\n",
        "        else:  # No transit\n",
        "            # Clean non-transit case\n",
        "            noise = np.random.normal(0, 0.001, len(t))\n",
        "            trend = np.random.uniform(-0.005, 0.005) * t  # Smaller trends\n",
        "            curve = 1 + noise + trend\n",
        "\n",
        "            curves[i] = curve\n",
        "            labels[i] = 0\n",
        "\n",
        "        # Normalize\n",
        "        curves[i] = (curves[i] - np.mean(curves[i])) / np.std(curves[i])\n",
        "\n",
        "    return curves, labels\n"
      ],
      "metadata": {
        "id": "GrAjG8jmTaFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Functions"
      ],
      "metadata": {
        "id": "vI2v3pcPNbvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_training_progress(model, dataloader, epoch, save_dir=None):\n",
        "    \"\"\"Visualize and save training progress\"\"\"\n",
        "    if save_dir is None:\n",
        "        save_dir = os.path.join(SAVE_DIR, 'training_progress')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(dataloader))\n",
        "        batch = batch.to(device)\n",
        "        reconstructions = model(batch)\n",
        "        n = min(8, batch.size(0))\n",
        "        comparison = torch.cat([batch[:n], reconstructions[:n]])\n",
        "        grid = make_grid(comparison.cpu(), nrow=n, normalize=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.imshow(grid.permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "        plt.savefig(os.path.join(save_dir, f'epoch_{epoch}.png'))\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "x54AQNcWNWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)"
      ],
      "metadata": {
        "id": "tj9wHsVSNa4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(model, train_loader, num_epochs=100):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Use SAVE_DIR for checkpoints\n",
        "    checkpoint_dir = os.path.join(SAVE_DIR, 'checkpoints')\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    # Split data into train and validation\n",
        "    train_size = int(0.8 * len(train_loader.dataset))\n",
        "    val_size = len(train_loader.dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch)\n",
        "            loss = criterion(output, batch)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                output = model(batch)\n",
        "                loss = criterion(output, batch)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if validation loss improved\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint(model, optimizer, epoch, avg_val_loss,\n",
        "                          os.path.join(checkpoint_dir, 'best_autoencoder.pth'))\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_checkpoint(model, optimizer, epoch, avg_train_loss,\n",
        "                          os.path.join(checkpoint_dir, f'autoencoder_epoch_{epoch+1}.pth'))\n",
        "\n",
        "        # Visualize progress\n",
        "        visualize_training_progress(model, train_loader, epoch)\n",
        "\n",
        "    # Plot and save training history\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, 'autoencoder_training_history.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return history\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qjg-ly3NT5WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, labels in dataloader:\n",
        "            batch, labels = batch.to(device), labels.to(device)\n",
        "            outputs = model(batch)\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds).squeeze()\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Calculate Precision-Recall curve\n",
        "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    ap = average_precision_score(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        'roc_auc': roc_auc,\n",
        "        'ap': ap,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "jSPVNjuGUUdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(metrics, save_dir=None):\n",
        "    if save_dir is None:\n",
        "        save_dir = os.path.join(SAVE_DIR, 'metrics')\n",
        "\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(metrics['fpr'], metrics['tpr'], color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {metrics[\"roc_auc\"]:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(metrics['recall'], metrics['precision'], color='blue', lw=2,\n",
        "             label=f'PR curve (AP = {metrics[\"ap\"]:.2f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'performance_curves.png'))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "9xmyMbluUV4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, train_loader, val_loader, num_epochs=100):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Better optimizer settings\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0.0003,\n",
        "        weight_decay=0.001,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # OneCycleLR scheduler\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=0.001,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.3,\n",
        "        div_factor=25.0,\n",
        "        final_div_factor=1000.0\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    max_patience = 15\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch)\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            predicted = (torch.sigmoid(output) > 0.5).float()\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted.squeeze() == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch, labels in val_loader:\n",
        "                batch, labels = batch.to(device), labels.to(device)\n",
        "                output = model(batch)\n",
        "                loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                predicted = (torch.sigmoid(output) > 0.5).float()\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted.squeeze() == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}:')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'val_acc': val_accuracy,\n",
        "            }, os.path.join(SAVE_DIR, 'best_classifier.pth'))\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= max_patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "xsE7AqPnUjjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Q0KW4IxUnmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    DATA_DIR = '/content/drive/MyDrive/continuum_data_subset'\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    disk_dataset = DiskDataset(DATA_DIR)\n",
        "    disk_loader = DataLoader(disk_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Initialize models\n",
        "    clustering_model = DiskClustering(n_clusters=5)\n",
        "    autoencoder = DiskAutoencoder().to(device)\n",
        "    classifier = TransitClassifier().to(device)\n",
        "\n",
        "    # Train models\n",
        "    print('Training Autoencoder...')\n",
        "    train_autoencoder(autoencoder, disk_loader)\n",
        "\n",
        "    print('Training Classifier...')\n",
        "    # Generate more data for better training\n",
        "    transit_curves, labels = generate_transit_curves(n_samples=5000)\n",
        "\n",
        "    # Create datasets\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        torch.FloatTensor(transit_curves),\n",
        "        torch.FloatTensor(labels)\n",
        "    )\n",
        "\n",
        "    # Split into train, validation, and test sets\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize and train model\n",
        "    history = train_classifier(classifier, train_loader, val_loader)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    classifier.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'best_classifier.pth'))['model_state_dict'])\n",
        "    test_metrics = evaluate_model(classifier, test_loader, device)\n",
        "    plot_metrics(test_metrics)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Val Accuracy')\n",
        "    plt.title('Accuracy History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, 'classifier_training_history.png'))\n",
        "    plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e84oxmuINm21",
        "outputId": "726c89dc-b264-4631-ede2-7cf85d82048b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Autoencoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 4/4 [00:06<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.0741, Val Loss: 0.0151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Train Loss: 0.0115, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Train Loss: 0.0115, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Train Loss: 0.0116, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Train Loss: 0.0118, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Train Loss: 0.0118, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Train Loss: 0.0116, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Train Loss: 0.0119, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Train Loss: 0.0119, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Train Loss: 0.0119, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Train Loss: 0.0118, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Train Loss: 0.0116, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Train Loss: 0.0118, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Train Loss: 0.0112, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21, Train Loss: 0.0115, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24, Train Loss: 0.0112, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28, Train Loss: 0.0117, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29, Train Loss: 0.0115, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Train Loss: 0.0116, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31, Train Loss: 0.0116, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 4/4 [00:08<00:00,  2.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32, Train Loss: 0.0117, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33, Train Loss: 0.0118, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34, Train Loss: 0.0111, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35, Train Loss: 0.0111, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36, Train Loss: 0.0112, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39, Train Loss: 0.0117, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Train Loss: 0.0115, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43, Train Loss: 0.0115, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46, Train Loss: 0.0114, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48, Train Loss: 0.0115, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49, Train Loss: 0.0113, Val Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Train Loss: 0.0113, Val Loss: 0.0150\n",
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 110/110 [00:01<00:00, 94.87it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "Train Loss: 0.5620, Train Acc: 64.00%\n",
            "Val Loss: 0.5265, Val Acc: 73.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 110/110 [00:00<00:00, 172.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2:\n",
            "Train Loss: 0.5385, Train Acc: 72.43%\n",
            "Val Loss: 0.5241, Val Acc: 72.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100: 100%|██████████| 110/110 [00:00<00:00, 165.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3:\n",
            "Train Loss: 0.5342, Train Acc: 75.94%\n",
            "Val Loss: 0.5192, Val Acc: 78.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/100: 100%|██████████| 110/110 [00:00<00:00, 166.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4:\n",
            "Train Loss: 0.5301, Train Acc: 80.43%\n",
            "Val Loss: 0.5184, Val Acc: 90.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/100: 100%|██████████| 110/110 [00:00<00:00, 168.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5:\n",
            "Train Loss: 0.5321, Train Acc: 82.11%\n",
            "Val Loss: 0.5235, Val Acc: 83.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/100: 100%|██████████| 110/110 [00:00<00:00, 170.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6:\n",
            "Train Loss: 0.5278, Train Acc: 82.94%\n",
            "Val Loss: 0.5177, Val Acc: 90.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/100: 100%|██████████| 110/110 [00:00<00:00, 169.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7:\n",
            "Train Loss: 0.5270, Train Acc: 82.74%\n",
            "Val Loss: 0.5166, Val Acc: 91.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100: 100%|██████████| 110/110 [00:00<00:00, 129.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8:\n",
            "Train Loss: 0.5238, Train Acc: 82.71%\n",
            "Val Loss: 0.5176, Val Acc: 85.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100: 100%|██████████| 110/110 [00:01<00:00, 107.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9:\n",
            "Train Loss: 0.5242, Train Acc: 81.86%\n",
            "Val Loss: 0.5163, Val Acc: 81.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100: 100%|██████████| 110/110 [00:00<00:00, 116.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10:\n",
            "Train Loss: 0.5236, Train Acc: 82.97%\n",
            "Val Loss: 0.5155, Val Acc: 85.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 110/110 [00:01<00:00, 103.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11:\n",
            "Train Loss: 0.5337, Train Acc: 80.60%\n",
            "Val Loss: 0.5204, Val Acc: 86.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100: 100%|██████████| 110/110 [00:01<00:00, 67.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12:\n",
            "Train Loss: 0.5321, Train Acc: 80.54%\n",
            "Val Loss: 0.5220, Val Acc: 79.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100: 100%|██████████| 110/110 [00:01<00:00, 66.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13:\n",
            "Train Loss: 0.5279, Train Acc: 80.60%\n",
            "Val Loss: 0.5231, Val Acc: 83.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100: 100%|██████████| 110/110 [00:01<00:00, 83.69it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14:\n",
            "Train Loss: 0.5252, Train Acc: 79.26%\n",
            "Val Loss: 0.5152, Val Acc: 80.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100: 100%|██████████| 110/110 [00:00<00:00, 110.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15:\n",
            "Train Loss: 0.5297, Train Acc: 81.03%\n",
            "Val Loss: 0.5223, Val Acc: 67.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100: 100%|██████████| 110/110 [00:00<00:00, 110.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16:\n",
            "Train Loss: 0.5275, Train Acc: 81.74%\n",
            "Val Loss: 0.5133, Val Acc: 84.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100: 100%|██████████| 110/110 [00:00<00:00, 110.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17:\n",
            "Train Loss: 0.5257, Train Acc: 82.43%\n",
            "Val Loss: 0.5182, Val Acc: 84.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100: 100%|██████████| 110/110 [00:01<00:00, 99.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18:\n",
            "Train Loss: 0.5246, Train Acc: 82.74%\n",
            "Val Loss: 0.5192, Val Acc: 76.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/100: 100%|██████████| 110/110 [00:00<00:00, 115.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19:\n",
            "Train Loss: 0.5234, Train Acc: 81.69%\n",
            "Val Loss: 0.5143, Val Acc: 86.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/100: 100%|██████████| 110/110 [00:01<00:00, 106.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20:\n",
            "Train Loss: 0.5218, Train Acc: 80.89%\n",
            "Val Loss: 0.5107, Val Acc: 76.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/100: 100%|██████████| 110/110 [00:01<00:00, 106.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21:\n",
            "Train Loss: 0.5230, Train Acc: 81.00%\n",
            "Val Loss: 0.5117, Val Acc: 80.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/100: 100%|██████████| 110/110 [00:01<00:00, 107.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22:\n",
            "Train Loss: 0.5195, Train Acc: 79.54%\n",
            "Val Loss: 0.5090, Val Acc: 76.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/100: 100%|██████████| 110/110 [00:01<00:00, 81.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23:\n",
            "Train Loss: 0.5188, Train Acc: 78.89%\n",
            "Val Loss: 0.5105, Val Acc: 75.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/100: 100%|██████████| 110/110 [00:01<00:00, 62.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24:\n",
            "Train Loss: 0.5195, Train Acc: 79.11%\n",
            "Val Loss: 0.5107, Val Acc: 79.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/100: 100%|██████████| 110/110 [00:01<00:00, 77.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25:\n",
            "Train Loss: 0.5185, Train Acc: 79.00%\n",
            "Val Loss: 0.5089, Val Acc: 67.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/100: 100%|██████████| 110/110 [00:01<00:00, 104.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26:\n",
            "Train Loss: 0.5183, Train Acc: 78.94%\n",
            "Val Loss: 0.5086, Val Acc: 66.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/100: 100%|██████████| 110/110 [00:01<00:00, 105.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27:\n",
            "Train Loss: 0.5166, Train Acc: 78.71%\n",
            "Val Loss: 0.5093, Val Acc: 74.27%\n",
            "Early stopping triggered at epoch 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSKOf9R0N7Y6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}