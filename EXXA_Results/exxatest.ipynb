{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "RUN_EXXA2 = True  # Unsupervised clustering of protoplanetary disks\n",
        "RUN_EXXA4 = True  # Autoencoder for latent space analysis\n",
        "RUN_EXXA3 = True  # Classifier for exoplanet transit light curves\n"
      ],
      "metadata": {
        "id": "lSlJj4QR8oyp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFNKJSUs8MdB",
        "outputId": "29fad492-90e2-4615-bd9b-e886de03dcd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas torch torchvision astropy pytorch-msssim scikit-learn matplotlib seaborn tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbCRV0Z48kIY",
        "outputId": "2f0ef917-1b48-45ea-976e-d07c4f35555b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.11/dist-packages (7.0.1)\n",
            "Collecting pytorch-msssim\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.11/dist-packages (from astropy) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.1.31.12.41.4 in /usr/local/lib/python3.11/dist-packages (from astropy) (0.2025.3.24.0.35.32)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy) (6.0.2)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-msssim\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-msssim-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from astropy.io import fits\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pytorch_msssim import ssim, ms_ssim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "SAVE_DIR = '/content/drive/MyDrive/EXXA_Results'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'training_progress'), exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'checkpoints'), exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'metrics'), exist_ok=True)"
      ],
      "metadata": {
        "id": "FFpvZAWs85AG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "id": "X8peO-X--RX5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1w1ZhSb-dlE",
        "outputId": "d85b3cc4-d6ae-4f1a-b024-93e3f58708dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Preprocessing\n"
      ],
      "metadata": {
        "id": "3oJ6PiUs-mU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiskDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.fits')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
        "        with fits.open(file_path) as hdul:\n",
        "            image = hdul[0].data[0]\n",
        "\n",
        "        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
        "\n",
        "        if len(image.shape) > 2:\n",
        "            image = image.squeeze()\n",
        "\n",
        "        image = image[np.newaxis, :, :]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return torch.FloatTensor(image)"
      ],
      "metadata": {
        "id": "oMDMrEL6-kUb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised Clustering Model\n"
      ],
      "metadata": {
        "id": "LaYgOwhx-xIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiskClustering:\n",
        "    def __init__(self, n_clusters=5):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.kmeans = KMeans(\n",
        "            n_clusters=n_clusters,\n",
        "            random_state=42,\n",
        "            n_init=50,  # Increased for better initialization\n",
        "            max_iter=1000,  # More iterations\n",
        "            tol=1e-6  # Tighter convergence\n",
        "        )\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def preprocess_images(self, images):\n",
        "        \"\"\"Improved preprocessing pipeline\"\"\"\n",
        "        # Flatten images\n",
        "        n_samples = images.shape[0]\n",
        "        flattened = images.reshape(n_samples, -1)\n",
        "\n",
        "        # Remove low variance features\n",
        "        std = np.std(flattened, axis=0)\n",
        "        high_var_mask = std > np.percentile(std, 10)\n",
        "        flattened = flattened[:, high_var_mask]\n",
        "\n",
        "        # Remove outliers\n",
        "        mean = np.mean(flattened, axis=0)\n",
        "        std = np.std(flattened, axis=0)\n",
        "        z_scores = np.abs((flattened - mean) / std)\n",
        "        outlier_mask = np.all(z_scores < 3, axis=1)  # Remove points with z-score > 3\n",
        "        flattened = flattened[outlier_mask]\n",
        "\n",
        "        # Scale features\n",
        "        scaled = self.scaler.fit_transform(flattened)\n",
        "\n",
        "        pca = PCA(n_components=0.95)\n",
        "        reduced = pca.fit_transform(scaled)\n",
        "\n",
        "        self.high_var_mask = high_var_mask\n",
        "        self.outlier_mask = outlier_mask\n",
        "        self.pca = pca\n",
        "\n",
        "        print(f\"Preprocessed data shape: {reduced.shape}\")\n",
        "        print(f\"Removed {np.sum(~outlier_mask)} outliers\")\n",
        "        print(f\"Kept {reduced.shape[1]} components explaining 95% variance\")\n",
        "\n",
        "        return reduced\n",
        "\n",
        "    def fit(self, images):\n",
        "        print(\"Preprocessing data...\")\n",
        "        self.processed_data = self.preprocess_images(images)\n",
        "\n",
        "        print(f\"Fitting KMeans with {self.n_clusters} clusters...\")\n",
        "        self.kmeans.fit(self.processed_data)\n",
        "\n",
        "        self.silhouette = silhouette_score(self.processed_data, self.kmeans.labels_)\n",
        "        print(f\"Silhouette Score: {self.silhouette:.3f}\")\n",
        "\n",
        "        scores = []\n",
        "        for k in range(2, 8):\n",
        "            kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "            labels_temp = kmeans_temp.fit_predict(self.processed_data)\n",
        "            score = silhouette_score(self.processed_data, labels_temp)\n",
        "            scores.append(score)\n",
        "            print(f\"Silhouette Score with {k} clusters: {score:.3f}\")\n",
        "\n",
        "        best_k = np.argmax(scores) + 2\n",
        "        if best_k != self.n_clusters:\n",
        "            print(f\"\\nNote: {best_k} clusters might give better results (score: {max(scores):.3f})\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, images):\n",
        "        processed = self.preprocess_images(images)\n",
        "        return self.kmeans.predict(processed)\n",
        "\n",
        "    def visualize_clusters(self, images, labels):\n",
        "        fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i in range(self.n_clusters):\n",
        "            cluster_images = images[labels == i]\n",
        "            if len(cluster_images) > 0:\n",
        "                axes[i].imshow(cluster_images[0], cmap='viridis')\n",
        "                axes[i].set_title(f'Cluster {i}')\n",
        "                axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7rhzGNMd-yNn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoencoder Model"
      ],
      "metadata": {
        "id": "GU-UQDqr-8n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiskAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DiskAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 75 * 75, latent_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128 * 75 * 75),\n",
        "            nn.Unflatten(1, (128, 75, 75)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        return self.decode(z)"
      ],
      "metadata": {
        "id": "eZXWogGA-5pN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transit Curve Classifier"
      ],
      "metadata": {
        "id": "CPte5jDN_FZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransitClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(TransitClassifier, self).__init__()\n",
        "\n",
        "        # Feature extraction with attention\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        features = self.features(x)\n",
        "\n",
        "        # Apply attention\n",
        "        attention_weights = self.attention(features)\n",
        "        attended_features = features * attention_weights\n",
        "\n",
        "        # Classification\n",
        "        return self.classifier(attended_features)\n",
        "\n",
        "def generate_transit_curves(n_samples=5000, n_points=100):\n",
        "    curves = np.zeros((n_samples, n_points))\n",
        "    labels = np.zeros(n_samples)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        t = np.linspace(0, 1, n_points)\n",
        "        if np.random.random() > 0.5:  # Transit present\n",
        "            depth = np.random.uniform(0.01, 0.05)  # Shallower transits\n",
        "            duration = np.random.uniform(0.05, 0.15)  # More varied duration\n",
        "            center = np.random.uniform(0.2, 0.8)  # More varied position\n",
        "\n",
        "            transit = np.ones_like(t)\n",
        "            mask = np.abs(t - center) < duration/2\n",
        "            transit[mask] = 1 - depth\n",
        "\n",
        "            # Add ingress/egress\n",
        "            ingress_duration = duration * 0.1\n",
        "            ingress_mask = (t > center - duration/2) & (t < center - duration/2 + ingress_duration)\n",
        "            egress_mask = (t > center + duration/2 - ingress_duration) & (t < center + duration/2)\n",
        "\n",
        "            # Smooth ingress/egress\n",
        "            for mask in [ingress_mask, egress_mask]:\n",
        "                if np.any(mask):\n",
        "                    transit[mask] = 1 - depth * (1 - np.abs(t[mask] - center) / (duration/2))\n",
        "\n",
        "            # More realistic noise and systematics\n",
        "            noise = np.random.normal(0, 0.005, len(t))  # Increased noise\n",
        "            trend = np.random.uniform(-0.01, 0.01) * t  # Larger trends\n",
        "            oscillation = 0.002 * np.sin(2 * np.pi * t * np.random.uniform(1, 3))  # Add oscillations\n",
        "\n",
        "            curve = transit + noise + trend + oscillation\n",
        "\n",
        "            curves[i] = curve\n",
        "            labels[i] = 1\n",
        "        else:  # No transit\n",
        "\n",
        "            noise = np.random.normal(0, 0.005, len(t))\n",
        "            trend = np.random.uniform(-0.01, 0.01) * t\n",
        "            oscillation = 0.002 * np.sin(2 * np.pi * t * np.random.uniform(1, 3))\n",
        "\n",
        "            curve = 1 + noise + trend + oscillation\n",
        "\n",
        "            curves[i] = curve\n",
        "            labels[i] = 0\n",
        "\n",
        "        # Normalize\n",
        "        curves[i] = (curves[i] - np.mean(curves[i])) / np.std(curves[i])\n",
        "\n",
        "    return curves, labels"
      ],
      "metadata": {
        "id": "9a4_E_R2_BlV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Functions"
      ],
      "metadata": {
        "id": "w8IaeLwW_SGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_training_progress(model, dataloader, epoch, save_dir=None):\n",
        "    if save_dir is None:\n",
        "        save_dir = os.path.join(SAVE_DIR, 'training_progress')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(dataloader))\n",
        "        batch = batch.to(device)\n",
        "        reconstructions = model(batch)\n",
        "        n = min(8, batch.size(0))\n",
        "        comparison = torch.cat([batch[:n], reconstructions[:n]])\n",
        "        grid = make_grid(comparison.cpu(), nrow=n, normalize=True)\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.imshow(grid.permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "        plt.savefig(os.path.join(save_dir, f'epoch_{epoch}.png'))\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "WUXAcYa__Ptr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)\n"
      ],
      "metadata": {
        "id": "NxB6AXg9_b2Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(model, train_loader, num_epochs=50):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    checkpoint_dir = os.path.join(SAVE_DIR, 'checkpoints')\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    train_size = int(0.8 * len(train_loader.dataset))\n",
        "    val_size = len(train_loader.dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch)\n",
        "            loss = criterion(output, batch)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                output = model(batch)\n",
        "                loss = criterion(output, batch)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if validation loss improved\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint(model, optimizer, epoch, avg_val_loss,\n",
        "                          os.path.join(checkpoint_dir, 'best_autoencoder.pth'))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_checkpoint(model, optimizer, epoch, avg_train_loss,\n",
        "                          os.path.join(checkpoint_dir, f'autoencoder_epoch_{epoch+1}.pth'))\n",
        "\n",
        "        visualize_training_progress(model, train_loader, epoch)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, 'autoencoder_training_history.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "9p5Y0kcq_iX7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "    auc_score = auc(fpr, tpr)\n",
        "\n",
        "    # Calculate precision-recall curve\n",
        "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    avg_precision = average_precision_score(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'auc': auc_score,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'avg_precision': avg_precision\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "8pPYy1uj_nbg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(metrics, save_dir=None):\n",
        "    \"\"\"Plot and save evaluation metrics.\"\"\"\n",
        "    if save_dir is None:\n",
        "        save_dir = os.path.join(SAVE_DIR, 'metrics')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(metrics['fpr'], metrics['tpr'], color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {metrics[\"auc\"]:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(os.path.join(save_dir, 'roc_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot precision-recall curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(metrics['recall'], metrics['precision'], color='blue', lw=2,\n",
        "             label=f'Precision-Recall curve (AP = {metrics[\"avg_precision\"]:.3f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(os.path.join(save_dir, 'precision_recall_curve.png'))\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "jOYVvJ0u_7og"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_classifier(model, train_loader, val_loader, num_epochs=100, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.001)\n",
        "\n",
        "    # Learning rate scheduler with warmup\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=0.001,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience = 20\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "\n",
        "            # Add L1 regularization\n",
        "            l1_lambda = 0.0001\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            train_correct += (preds.squeeze() == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.sigmoid(outputs) > 0.5\n",
        "                val_correct += (preds.squeeze() == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            model_path = os.path.join(SAVE_DIR, 'checkpoints', 'best_classifier.pth')\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "                break\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "9XELV5Iz__UZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_clustering_results(model, data, labels, save_dir=None):\n",
        "    \"\"\"Generate and save clustering visualizations.\"\"\"\n",
        "    if save_dir is None:\n",
        "        save_dir = os.path.join(SAVE_DIR, 'clustering')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    data_2d = model.processed_data\n",
        "    labels = model.kmeans.labels_\n",
        "\n",
        "    # 1. PCA visualization\n",
        "    print(\"Generating PCA visualization...\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('Cluster Visualization (PCA)')\n",
        "    plt.xlabel('First Principal Component')\n",
        "    plt.ylabel('Second Principal Component')\n",
        "    plt.savefig(os.path.join(save_dir, 'cluster_visualization.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Silhouette plot\n",
        "    print(\"Generating Silhouette plot...\")\n",
        "    silhouette_avg = silhouette_score(data_2d, labels)\n",
        "    sample_silhouette_values = silhouette_samples(data_2d, labels)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    y_lower = 10\n",
        "    for i in range(len(set(labels))):\n",
        "        ith_cluster_values = sample_silhouette_values[labels == i]\n",
        "        ith_cluster_values.sort()\n",
        "        size_cluster_i = len(ith_cluster_values)\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = plt.cm.nipy_spectral(float(i) / len(set(labels)))\n",
        "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                         0, ith_cluster_values,\n",
        "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        y_lower = y_upper + 10\n",
        "\n",
        "    plt.title(f'Silhouette Analysis (avg score: {silhouette_avg:.3f})')\n",
        "    plt.xlabel('Silhouette Coefficient')\n",
        "    plt.ylabel('Cluster')\n",
        "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    plt.savefig(os.path.join(save_dir, 'silhouette_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Clustering visualizations saved in {save_dir}\")\n",
        "    return silhouette_avg"
      ],
      "metadata": {
        "id": "F4RvGbnuPrzS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_autoencoder_results(model, dataloader, save_dir='/content/drive/MyDrive/EXXA_Results/autoencoder'):\n",
        "    \"\"\"Visualize autoencoder results\"\"\"\n",
        "    model.eval()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    data_iter = iter(dataloader)\n",
        "    samples = next(data_iter)\n",
        "    samples = samples.to(next(model.parameters()).device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        reconstructions = model(samples)\n",
        "\n",
        "    samples = samples.cpu().numpy()\n",
        "    reconstructions = reconstructions.cpu().numpy()\n",
        "\n",
        "    # Plot original vs reconstructed images\n",
        "    n_samples = min(5, len(samples))\n",
        "    fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Original\n",
        "        axes[0, i].imshow(samples[i, 0], cmap='viridis')\n",
        "        axes[0, i].set_title('Original')\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Reconstructed\n",
        "        axes[1, i].imshow(reconstructions[i, 0], cmap='viridis')\n",
        "        axes[1, i].set_title('Reconstructed')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'reconstruction_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot loss distribution\n",
        "    if hasattr(model, 'train_losses') and hasattr(model, 'val_losses'):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(model.train_losses, label='Training Loss')\n",
        "        plt.plot(model.val_losses, label='Validation Loss')\n",
        "        plt.title('Autoencoder Training Progress')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(save_dir, 'training_progress.png'))\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "yxPIRLj0Z_uJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    DATA_DIR = '/content/drive/MyDrive/continuum_data_subset'\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    disk_dataset = DiskDataset(DATA_DIR)\n",
        "    disk_loader = DataLoader(disk_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize models\n",
        "    if RUN_EXXA2:\n",
        "        print('Running EXXA2: Unsupervised Clustering')\n",
        "\n",
        "        # Initialize clustering model with 2 clusters instead of 5\n",
        "        clustering_model = DiskClustering(n_clusters=2)\n",
        "\n",
        "        print(\"Loading disk data...\")\n",
        "        all_data = []\n",
        "        for batch in disk_loader:\n",
        "            all_data.append(batch.numpy())\n",
        "        disk_data = np.concatenate(all_data, axis=0)\n",
        "        print(f\"Loaded disk data shape: {disk_data.shape}\")\n",
        "\n",
        "        # Fit the clustering model and get labels\n",
        "        print(\"Fitting clustering model...\")\n",
        "        clustering_model.fit(disk_data)\n",
        "        cluster_labels = clustering_model.kmeans.labels_\n",
        "\n",
        "        # Generate visualizations\n",
        "        print(\"Generating clustering visualizations...\")\n",
        "        visualize_clustering_results(clustering_model, disk_data, cluster_labels)\n",
        "    if RUN_EXXA4:\n",
        "        print('Running EXXA4: Autoencoder Training...')\n",
        "        autoencoder = DiskAutoencoder().to(device)\n",
        "        train_autoencoder(autoencoder, disk_loader)\n",
        "\n",
        "        # Generate autoencoder visualizations\n",
        "        print(\"Generating autoencoder visualizations...\")\n",
        "        visualize_autoencoder_results(autoencoder, disk_loader)\n",
        "\n",
        "    if RUN_EXXA3:\n",
        "        print('Running EXXA3: Transit Classifier Training...')\n",
        "        # Generate more data for better training\n",
        "        transit_curves, labels = generate_transit_curves(n_samples=10000)  # Increased samples\n",
        "\n",
        "        # Create datasets\n",
        "        dataset = torch.utils.data.TensorDataset(\n",
        "            torch.FloatTensor(transit_curves),\n",
        "            torch.FloatTensor(labels)\n",
        "        )\n",
        "\n",
        "        # Split into train, validation, and test sets\n",
        "        train_size = int(0.7 * len(dataset))\n",
        "        val_size = int(0.15 * len(dataset))\n",
        "        test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "            dataset, [train_size, val_size, test_size]\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Increased batch size\n",
        "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Initialize and train model\n",
        "        classifier = TransitClassifier(100).to(device)\n",
        "        history = train_classifier(classifier, train_loader, val_loader, num_epochs=150)  # More epochs\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model_path = os.path.join(SAVE_DIR, 'checkpoints', 'best_classifier.pth')\n",
        "        torch.save(classifier.state_dict(), model_path)\n",
        "        classifier.load_state_dict(torch.load(model_path))\n",
        "        test_metrics = evaluate_model(classifier, test_loader, device)\n",
        "        plot_metrics(test_metrics)\n",
        "        print(f'Test AUC: {test_metrics[\"auc\"]:.3f}')\n",
        "        print(f'Test Average Precision: {test_metrics[\"avg_precision\"]:.3f}')\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history['train_loss'], label='Train Loss')\n",
        "        plt.plot(history['val_loss'], label='Val Loss')\n",
        "        plt.title('Loss History')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "        plt.plot(history['val_acc'], label='Val Accuracy')\n",
        "        plt.title('Accuracy History')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, 'classifier_training_history.png'))\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxz6co07AFv-",
        "outputId": "ded551b8-36ee-4329-e457-8b24a6dd4bef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running EXXA2: Unsupervised Clustering\n",
            "Loading disk data...\n",
            "Loaded disk data shape: (150, 1, 600, 600)\n",
            "Fitting clustering model...\n",
            "Preprocessing data...\n",
            "Preprocessed data shape: (59, 7)\n",
            "Removed 91 outliers\n",
            "Kept 7 components explaining 95% variance\n",
            "Fitting KMeans with 2 clusters...\n",
            "Silhouette Score: 0.433\n",
            "Silhouette Score with 2 clusters: 0.433\n",
            "Silhouette Score with 3 clusters: 0.403\n",
            "Silhouette Score with 4 clusters: 0.307\n",
            "Silhouette Score with 5 clusters: 0.329\n",
            "Silhouette Score with 6 clusters: 0.370\n",
            "Silhouette Score with 7 clusters: 0.358\n",
            "Generating clustering visualizations...\n",
            "Generating PCA visualization...\n",
            "Generating Silhouette plot...\n",
            "Clustering visualizations saved in /content/drive/MyDrive/EXXA_Results/clustering\n",
            "Running EXXA4: Autoencoder Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.2224, Val Loss: 0.1260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Train Loss: 0.0588, Val Loss: 0.0178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Train Loss: 0.0108, Val Loss: 0.0174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Train Loss: 0.0113, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Train Loss: 0.0115, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Train Loss: 0.0113, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Train Loss: 0.0107, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Train Loss: 0.0107, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Train Loss: 0.0111, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Train Loss: 0.0111, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Train Loss: 0.0112, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Train Loss: 0.0111, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Train Loss: 0.0107, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24, Train Loss: 0.0111, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26, Train Loss: 0.0107, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32, Train Loss: 0.0106, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34, Train Loss: 0.0106, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35, Train Loss: 0.0111, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36, Train Loss: 0.0112, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37, Train Loss: 0.0107, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41, Train Loss: 0.0107, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42, Train Loss: 0.0113, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 4/4 [00:06<00:00,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43, Train Loss: 0.0110, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44, Train Loss: 0.0112, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45, Train Loss: 0.0111, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46, Train Loss: 0.0111, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47, Train Loss: 0.0108, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49, Train Loss: 0.0109, Val Loss: 0.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Train Loss: 0.0111, Val Loss: 0.0173\n",
            "Generating autoencoder visualizations...\n",
            "Running EXXA3: Transit Classifier Training...\n",
            "Epoch [1/150]\n",
            "Train Loss: 2.6150, Train Acc: 52.13%\n",
            "Val Loss: 0.6604, Val Acc: 60.13%\n",
            "Epoch [2/150]\n",
            "Train Loss: 2.5577, Train Acc: 55.83%\n",
            "Val Loss: 0.5881, Val Acc: 72.67%\n",
            "Epoch [3/150]\n",
            "Train Loss: 2.4651, Train Acc: 64.19%\n",
            "Val Loss: 0.4965, Val Acc: 78.60%\n",
            "Epoch [4/150]\n",
            "Train Loss: 2.3477, Train Acc: 73.11%\n",
            "Val Loss: 0.4043, Val Acc: 80.80%\n",
            "Epoch [5/150]\n",
            "Train Loss: 2.2242, Train Acc: 80.46%\n",
            "Val Loss: 0.3358, Val Acc: 84.07%\n",
            "Epoch [6/150]\n",
            "Train Loss: 2.1292, Train Acc: 84.69%\n",
            "Val Loss: 0.2565, Val Acc: 88.47%\n",
            "Epoch [7/150]\n",
            "Train Loss: 2.0177, Train Acc: 87.79%\n",
            "Val Loss: 0.2081, Val Acc: 91.60%\n",
            "Epoch [8/150]\n",
            "Train Loss: 1.9129, Train Acc: 90.87%\n",
            "Val Loss: 0.1556, Val Acc: 94.73%\n",
            "Epoch [9/150]\n",
            "Train Loss: 1.8111, Train Acc: 92.27%\n",
            "Val Loss: 0.1346, Val Acc: 95.07%\n",
            "Epoch [10/150]\n",
            "Train Loss: 1.7081, Train Acc: 93.74%\n",
            "Val Loss: 0.1270, Val Acc: 96.00%\n",
            "Epoch [11/150]\n",
            "Train Loss: 1.6038, Train Acc: 94.59%\n",
            "Val Loss: 0.1190, Val Acc: 95.67%\n",
            "Epoch [12/150]\n",
            "Train Loss: 1.5049, Train Acc: 95.13%\n",
            "Val Loss: 0.1160, Val Acc: 95.80%\n",
            "Epoch [13/150]\n",
            "Train Loss: 1.3944, Train Acc: 95.94%\n",
            "Val Loss: 0.1055, Val Acc: 96.47%\n",
            "Epoch [14/150]\n",
            "Train Loss: 1.3015, Train Acc: 95.89%\n",
            "Val Loss: 0.1091, Val Acc: 96.13%\n",
            "Epoch [15/150]\n",
            "Train Loss: 1.2082, Train Acc: 96.31%\n",
            "Val Loss: 0.1032, Val Acc: 96.67%\n",
            "Epoch [16/150]\n",
            "Train Loss: 1.1235, Train Acc: 96.77%\n",
            "Val Loss: 0.0970, Val Acc: 96.87%\n",
            "Epoch [17/150]\n",
            "Train Loss: 1.0548, Train Acc: 96.63%\n",
            "Val Loss: 0.1047, Val Acc: 96.60%\n",
            "Epoch [18/150]\n",
            "Train Loss: 0.9934, Train Acc: 96.63%\n",
            "Val Loss: 0.1122, Val Acc: 96.73%\n",
            "Epoch [19/150]\n",
            "Train Loss: 0.9292, Train Acc: 97.23%\n",
            "Val Loss: 0.1002, Val Acc: 96.87%\n",
            "Epoch [20/150]\n",
            "Train Loss: 0.8760, Train Acc: 97.19%\n",
            "Val Loss: 0.0934, Val Acc: 97.27%\n",
            "Epoch [21/150]\n",
            "Train Loss: 0.8218, Train Acc: 97.43%\n",
            "Val Loss: 0.0976, Val Acc: 96.67%\n",
            "Epoch [22/150]\n",
            "Train Loss: 0.7836, Train Acc: 97.59%\n",
            "Val Loss: 0.1147, Val Acc: 96.40%\n",
            "Epoch [23/150]\n",
            "Train Loss: 0.7433, Train Acc: 97.47%\n",
            "Val Loss: 0.1009, Val Acc: 96.53%\n",
            "Epoch [24/150]\n",
            "Train Loss: 0.7144, Train Acc: 97.66%\n",
            "Val Loss: 0.1049, Val Acc: 96.87%\n",
            "Epoch [25/150]\n",
            "Train Loss: 0.6907, Train Acc: 97.54%\n",
            "Val Loss: 0.1005, Val Acc: 97.00%\n",
            "Epoch [26/150]\n",
            "Train Loss: 0.6596, Train Acc: 97.66%\n",
            "Val Loss: 0.1006, Val Acc: 96.80%\n",
            "Epoch [27/150]\n",
            "Train Loss: 0.6365, Train Acc: 97.60%\n",
            "Val Loss: 0.1212, Val Acc: 96.40%\n",
            "Epoch [28/150]\n",
            "Train Loss: 0.6265, Train Acc: 97.36%\n",
            "Val Loss: 0.1121, Val Acc: 97.07%\n",
            "Epoch [29/150]\n",
            "Train Loss: 0.5956, Train Acc: 97.80%\n",
            "Val Loss: 0.0923, Val Acc: 97.07%\n",
            "Epoch [30/150]\n",
            "Train Loss: 0.5797, Train Acc: 97.60%\n",
            "Val Loss: 0.1103, Val Acc: 96.87%\n",
            "Epoch [31/150]\n",
            "Train Loss: 0.5750, Train Acc: 97.54%\n",
            "Val Loss: 0.1145, Val Acc: 96.60%\n",
            "Epoch [32/150]\n",
            "Train Loss: 0.5487, Train Acc: 97.84%\n",
            "Val Loss: 0.1394, Val Acc: 96.67%\n",
            "Epoch [33/150]\n",
            "Train Loss: 0.5297, Train Acc: 98.19%\n",
            "Val Loss: 0.1099, Val Acc: 96.20%\n",
            "Epoch [34/150]\n",
            "Train Loss: 0.5291, Train Acc: 97.76%\n",
            "Val Loss: 0.1181, Val Acc: 96.13%\n",
            "Epoch [35/150]\n",
            "Train Loss: 0.5094, Train Acc: 98.00%\n",
            "Val Loss: 0.1168, Val Acc: 96.80%\n",
            "Epoch [36/150]\n",
            "Train Loss: 0.5085, Train Acc: 97.71%\n",
            "Val Loss: 0.0991, Val Acc: 97.00%\n",
            "Epoch [37/150]\n",
            "Train Loss: 0.4885, Train Acc: 98.00%\n",
            "Val Loss: 0.1094, Val Acc: 97.20%\n",
            "Epoch [38/150]\n",
            "Train Loss: 0.4803, Train Acc: 98.04%\n",
            "Val Loss: 0.1028, Val Acc: 96.60%\n",
            "Epoch [39/150]\n",
            "Train Loss: 0.4647, Train Acc: 98.10%\n",
            "Val Loss: 0.1153, Val Acc: 96.60%\n",
            "Epoch [40/150]\n",
            "Train Loss: 0.4610, Train Acc: 98.06%\n",
            "Val Loss: 0.1124, Val Acc: 96.00%\n",
            "Early stopping triggered after 40 epochs\n",
            "Test AUC: 0.993\n",
            "Test Average Precision: 0.994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRYUBblxALMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}